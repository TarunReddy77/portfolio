{"status":"ok","feed":{"url":"https://medium.com/feed/@tarutornado","title":"Stories by TARUN REDDY on Medium","link":"https://medium.com/@tarutornado?source=rss-28003df0d9f4------2","author":"","description":"Stories by TARUN REDDY on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*AM5ZDsBLDPuUmeBaAlg0SA.jpeg"},"items":[{"title":"PyTorch functions","pubDate":"2021-07-29 13:45:52","link":"https://medium.com/@tarutornado/pytorch-functions-f8ad67c2779b?source=rss-28003df0d9f4------2","guid":"https://medium.com/p/f8ad67c2779b","author":"TARUN REDDY","thumbnail":"","description":"\n<h3>5 cool PyTorch functions that are good to\u00a0know!</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*x-GzLq7XW6JpTzvNKSO2cg.jpeg\"></figure><p>Pytorch is one of the most popular Deep Learning libraries used globally, packed with a ton of extremely useful functionality that comes built-in and is very easy to use. Most of the functions are frequently used by the Data Science community and hence are an integral part of the\u00a0library.</p>\n<p>In this notebook, we will be looking at the following 5 interesting functions in the PyTorch\u00a0library.</p>\n<ul>\n<li>polar</li>\n<li>full_like</li>\n<li>bernoulli</li>\n<li>topk</li>\n<li>combinations</li>\n</ul>\n<p>For each function, we\u2019ll be giving 3 examples, 2 that work and one that doesn\u2019t to help us understand better. Before we begin, let\u2019s install and import\u00a0PyTorch.</p>\n<pre># Uncomment and run the appropriate command for your operating system, if required<br><br># Linux / Binder<br># !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html<br><br># Windows<br># !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html<br><br># MacOS<br># !pip install numpy torch torchvision torchaudio</pre>\n<pre># Import torch and other required modules<br>import torch<br>import numpy as np</pre>\n<h4>Function 1\u200a\u2014\u200atorch.polar</h4>\n<p>The <strong>torch.polar</strong> function is used to create a tensor of <strong>polar complex numbers</strong>. It takes two tensors as arguments. The first contains the <strong>magnitudes</strong> of the complex numbers to be generated, while the second one contains the\u00a0<strong>angles</strong>.</p>\n<p>The function zips the two tensors together, creating complex numbers by taking the magnitude from tensor 1 and angle from tensor\u00a02.</p>\n<p>For example, the complex number at index k is generated by zipping the numbers at index 2 in the magnitude and angle tensors\u00a0like;</p>\n<p><strong>c[k] = a[k]cos(b[k]) + i(a[k]sin(b[k]))</strong>,</p>\n<p>where <strong>a[k]</strong> is the tensor of magnitudes, <strong>b[k]</strong> is the tensor of angles and <strong>c[k]</strong> is the resultant tensor of polar complex\u00a0numbers.</p>\n<p>Let\u2019s create a complex 1-dimensional tensor using the function.</p>\n<pre># Example 1<br>magnitudes = torch.tensor([1., 2])<br>angles = torch.tensor([0.25, 0.5])<br><br>complex_nums = torch.polar(magnitudes, angles)<br>complex_nums</pre>\n<pre>tensor([0.9689+0.2474j, 1.7552+0.9589j])</pre>\n<p>That worked! Let\u2019s see if we could extend this to higher dimensions as\u00a0well.</p>\n<pre># Example 2 - working<br>magnitudes = torch.tensor([[1, 2], [3, 4.]])<br>pi = np.pi<br>angles = torch.tensor([[pi/3, pi/2], [pi/4, pi/6]])<br><br>complex_nums = torch.polar(magnitudes, angles)<br>complex_nums</pre>\n<pre>tensor([[ 5.0000e-01+0.8660j, -8.7423e-08+2.0000j],<br>        [ 2.1213e+00+2.1213j,  3.4641e+00+2.0000j]])</pre>\n<p>Now, let\u2019s see what happens if the pass data of different data types to the two constituent tensors.</p>\n<pre># Example 3 - breaking<br>magnitudes = torch.tensor([[5, 12], [3, 41]], dtype = torch.float32)<br>pi = np.pi<br>angles = torch.tensor([[pi/3, pi/2], [pi/4, pi/6]], dtype = torch.float64)<br><br>complex_nums = torch.polar(magnitudes, angles)<br>complex_nums</pre>\n<pre>---------------------------------------------------------------------------<br><br>RuntimeError                              Traceback (most recent call last)<br><br>&lt;ipython-input-4-2bb46c724600&gt; in &lt;module&gt;<br>      4 angles = torch.tensor([[pi/3, pi/2], [pi/4, pi/6]], dtype = torch.float64)<br>      5 <br>----&gt; 6 complex_nums = torch.polar(magnitudes, angles)<br>      7 complex_nums<br><br><br>RuntimeError: Expected object of scalar type Float but got scalar type Double for second argument</pre>\n<p>Well, that didn\u2019t work! So, the torch.polar function expects the tensors to have the same data\u00a0types.</p>\n<p>This function is used to generate polar forms of complex numbers, which are frequently used in signal processing, which is closely related to machine learning.</p>\n<h4>Function 2\u200a\u2014\u200atorch.full_like</h4>\n<p>This function is used to generate a tensor of same dimensions as the given tensor, but filled with the value provided.</p>\n<p>In the following example, we\u2019ll create a new tensor with the same dimensions as\u00a0x.</p>\n<pre># Example 1 - working<br>x = torch.tensor([[1.,2],[3,4]])<br>y = torch.full_like(x,5)<br>y</pre>\n<pre>tensor([[5., 5.],<br>        [5., 5.]])</pre>\n<p>We see that y is of same dimensions as x, but filled with the value 5. It also inferred the same datatype from\u00a0x.</p>\n<p>Next, we\u2019ll try to explicitly mention the datatype of the new\u00a0tensor.</p>\n<pre># Example 2 - working<br>x = torch.tensor([[1.,2],[3,4]])<br>y = torch.full_like(x,5, dtype = torch.int32)<br>y</pre>\n<pre>tensor([[5, 5],<br>        [5, 5]], dtype=torch.int32)</pre>\n<p>Now, dtype is as we have instructed. Let us also see if the fill_value can be inferred by the example tensor, i.e, x. We\u2019ll not pass any fill_value.</p>\n<pre># Example 3 - breaking (to illustrate when it breaks)<br>x = torch.tensor([[1,1],[1,1]])<br>y = torch.full_like(x)<br>y</pre>\n<pre>---------------------------------------------------------------------------<br><br>TypeError                                 Traceback (most recent call last)<br><br>&lt;ipython-input-7-d44312400574&gt; in &lt;module&gt;<br>      1 # Example 3 - breaking (to illustrate when it breaks)<br>      2 x = torch.tensor([[1,1],[1,1]])<br>----&gt; 3 y = torch.full_like(x)<br>      4 y<br><br><br>TypeError: full_like() missing 1 required positional arguments: \"fill_value\"</pre>\n<p>We observe that fill value is a required argument and can\u2019t be inferred from x, despite x being filled with just 1\u00a0value.</p>\n<p>This function is extremely useful to create tensors that duplicate the dimensions of a particular tensor, but are initialized with a different value. It is especially used when the dimensions of the tensor to mimic are\u00a0unknown.</p>\n<h4>Function 3\u200a\u2014\u200atorch.bernoulli</h4>\n<p>This function is used to generate a tensor of binary numbers (i.e, 0s and 1s) using another tensor specifying the probabilities of choosing 1 in a bernoulli distribution.</p>\n<p>It takes a tensor of probabilities as input and outputs a tensor of binary numbers of the same dimensions as the\u00a0input.</p>\n<pre># Example 1 - working<br><br>a = torch.tensor([0.342, 0.765, 0.913, 0.245, 0.486])    #Let's create a tensor of probablities.<br>torch.bernoulli(a)</pre>\n<pre>tensor([1., 1., 1., 0., 1.])</pre>\n<p>We see that as expected, high probabilities correspond to 1s and low end up in 0s. The last probability being close to 0.5, could have ended up almost equally likely in either 0 or\u00a01.</p>\n<pre># Example 2 - working<br>a = torch.ones(3, 3)          # A tensor full of ones, implying that the probability of drawing \"1\" is 1<br>torch.bernoulli(a)</pre>\n<pre>tensor([[1., 1., 1.],<br>        [1., 1., 1.],<br>        [1., 1., 1.]])</pre>\n<p>In this example, the input tensor is full of ones. Therefore, the output tensor is also full of ones as the probability of choosing \u201c0\u201d is\u00a00.</p>\n<pre># Example 3 - breaking<br>a = torch.tensor([0.5, 0.2, 1.2, 0.9, 0.6])<br>torch.bernoulli(a)</pre>\n<pre>---------------------------------------------------------------------------<br><br>RuntimeError                              Traceback (most recent call last)<br><br>&lt;ipython-input-10-4476b721d566&gt; in &lt;module&gt;<br>      1 # Example 3 - breaking<br>      2 a = torch.tensor([0.5, 0.2, 1.2, 0.9, 0.6])<br>----&gt; 3 torch.bernoulli(a)<br><br><br>RuntimeError: Expected p_in &gt;= 0 &amp;&amp; p_in &lt;= 1 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)</pre>\n<p>Here, the input tensor contains the value \u201c1.2\u201d at index \u201c2\u201d, which is not a valid value for probability and therefore, the code breaks. Hence, we must ensure that all the values in the input tensor are in the range of 0 to 1 before feeding it to the torch.bernoulli function.</p>\n<p>torch.bernoulli is used to generate a bernoulli distribution of binary\u00a0numbers.</p>\n<h4>Function 4\u200a\u2014\u200atorch.topk</h4>\n<p>torch.topk function is used to find the \u2018k\u2019 largest elements in a tensor, along with their indices. It takes a tensor and a number \u2018k\u2019 as arguments.</p>\n<pre># Example 1 - working<br>a = torch.linspace(-1, 1, steps=4)    # A tensor of 4 equally spaced values in the range of [-1, 1]<br>print(a)<br>torch.topk(a, 3)</pre>\n<pre>tensor([-1.0000, -0.3333,  0.3333,  1.0000])<br><br><br><br><br><br>torch.return_types.topk(<br>values=tensor([ 1.0000,  0.3333, -0.3333]),<br>indices=tensor([3, 2, 1]))</pre>\n<p>As expected, the function returns the 3 largest numbers and their respective indices.</p>\n<pre># Example 2 - working<br>x = torch.rand(5,5)<br>print(x)<br>torch.topk(x, 3, largest = False)</pre>\n<pre>tensor([[0.9309, 0.9593, 0.3084, 0.8251, 0.5996],<br>        [0.4073, 0.1299, 0.5344, 0.2100, 0.7051],<br>        [0.5291, 0.6243, 0.2936, 0.1113, 0.4588],<br>        [0.6389, 0.6557, 0.4525, 0.3430, 0.2369],<br>        [0.6534, 0.3255, 0.9739, 0.2819, 0.3420]])<br><br><br><br><br><br>torch.return_types.topk(<br>values=tensor([[0.3084, 0.5996, 0.8251],<br>        [0.1299, 0.2100, 0.4073],<br>        [0.1113, 0.2936, 0.4588],<br>        [0.2369, 0.3430, 0.4525],<br>        [0.2819, 0.3255, 0.3420]]),<br>indices=tensor([[2, 4, 3],<br>        [1, 3, 0],<br>        [3, 2, 4],<br>        [4, 3, 2],<br>        [3, 1, 4]]))</pre>\n<p>In this example, we pass a 5x5 array to the topk function, asking for the smallest three numbers (since largest = False) along each row (since dim = 0 by default).</p>\n<p>We get the expected numbers along with their corresponding indices.</p>\n<pre># Example 3 - breaking<br>x = torch.arange(6.)     # A tensor of numbers in range [0,5]<br>print(x)<br>torch.topk(x,7)</pre>\n<pre>tensor([0., 1., 2., 3., 4., 5.])<br><br><br><br>---------------------------------------------------------------------------<br><br>RuntimeError                              Traceback (most recent call last)<br><br>&lt;ipython-input-13-200600ac317b&gt; in &lt;module&gt;<br>      2 x = torch.arange(6.)     # A tensor of numbers in range [0,5]<br>      3 print(x)<br>----&gt; 4 torch.topk(x,7)<br><br><br>RuntimeError: selected index k out of range</pre>\n<p>In this example, we ask for \u20187\u2019 largest numbers in a tensor containing only \u20186\u2019 values. Hence, the code\u00a0breaks.</p>\n<p>This function is pretty useful when we are only interested in either the few largest or smallest numbers in a tensor, irrespective of its\u00a0size.</p>\n<h4>Function 5\u200a\u2014\u200atorch.combinations</h4>\n<p>This function is used to generate a tensor of all combinations of size \u2018r\u2019 from a tensor. By default, it does it without replacements.</p>\n<pre># Example 1 - working<br>a = torch.tensor([1,2,3,4])<br>torch.combinations(a,3)</pre>\n<pre>tensor([[1, 2, 3],<br>        [1, 2, 4],<br>        [1, 3, 4],<br>        [2, 3, 4]])</pre>\n<p>It generated the 4 possible combinations of 3 elements from the given tensor as expected.</p>\n<pre># Example 2 - working<br>a = torch.tensor([1,2,3])<br>torch.combinations(a,2, with_replacement = True)</pre>\n<pre>tensor([[1, 1],<br>        [1, 2],<br>        [1, 3],<br>        [2, 2],<br>        [2, 3],<br>        [3, 3]])</pre>\n<p>Now, with \u2018with_replacement\u2019 parameter set to True, we see that replacements are allowed, giving many more combinations.</p>\n<pre># Example 3 - breaking<br>a = torch.tensor([[1,2,3], [4,5,6]])<br>torch.combinations(a,2)</pre>\n<pre>---------------------------------------------------------------------------<br><br>RuntimeError                              Traceback (most recent call last)<br><br>&lt;ipython-input-16-a9568cd81328&gt; in &lt;module&gt;<br>      1 # Example 3 - breaking<br>      2 a = torch.tensor([[1,2,3], [4,5,6]])<br>----&gt; 3 torch.combinations(a,2)<br><br><br>RuntimeError: Expect a 1D vector, but got shape [2, 3]</pre>\n<p>Here, when we provided a multi-dimensional vector as input, it is ambiguous as to how to choose elements for combinations, and hence, the code\u00a0breaks.</p>\n<p>This function is very useful to create all possible combinations of elements present in the input tensor, which is a common use case in discrete\u00a0math.</p>\n<h4>Conclusion</h4>\n<p>In this notebook, we had a sneak peek at 5 cool and useful functions in PyTorch. These functions are pretty useful for programmers working in Machine Learning to quickly generate the data in the way they want without worrying about the underlying logic involved.</p>\n<p>There are many more such functions in PyTorch, which can be explored in the PyTorch documentation, the link to which is provided below. Keep exploring!</p>\n<h4>Reference Link</h4>\n<ul><li>Official documentation for tensor operations: <a href=\"https://pytorch.org/docs/stable/torch.html\">https://pytorch.org/docs/stable/torch.html</a>\n</li></ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f8ad67c2779b\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>5 cool PyTorch functions that are good to\u00a0know!</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*x-GzLq7XW6JpTzvNKSO2cg.jpeg\"></figure><p>Pytorch is one of the most popular Deep Learning libraries used globally, packed with a ton of extremely useful functionality that comes built-in and is very easy to use. Most of the functions are frequently used by the Data Science community and hence are an integral part of the\u00a0library.</p>\n<p>In this notebook, we will be looking at the following 5 interesting functions in the PyTorch\u00a0library.</p>\n<ul>\n<li>polar</li>\n<li>full_like</li>\n<li>bernoulli</li>\n<li>topk</li>\n<li>combinations</li>\n</ul>\n<p>For each function, we\u2019ll be giving 3 examples, 2 that work and one that doesn\u2019t to help us understand better. Before we begin, let\u2019s install and import\u00a0PyTorch.</p>\n<pre># Uncomment and run the appropriate command for your operating system, if required<br><br># Linux / Binder<br># !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html<br><br># Windows<br># !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html<br><br># MacOS<br># !pip install numpy torch torchvision torchaudio</pre>\n<pre># Import torch and other required modules<br>import torch<br>import numpy as np</pre>\n<h4>Function 1\u200a\u2014\u200atorch.polar</h4>\n<p>The <strong>torch.polar</strong> function is used to create a tensor of <strong>polar complex numbers</strong>. It takes two tensors as arguments. The first contains the <strong>magnitudes</strong> of the complex numbers to be generated, while the second one contains the\u00a0<strong>angles</strong>.</p>\n<p>The function zips the two tensors together, creating complex numbers by taking the magnitude from tensor 1 and angle from tensor\u00a02.</p>\n<p>For example, the complex number at index k is generated by zipping the numbers at index 2 in the magnitude and angle tensors\u00a0like;</p>\n<p><strong>c[k] = a[k]cos(b[k]) + i(a[k]sin(b[k]))</strong>,</p>\n<p>where <strong>a[k]</strong> is the tensor of magnitudes, <strong>b[k]</strong> is the tensor of angles and <strong>c[k]</strong> is the resultant tensor of polar complex\u00a0numbers.</p>\n<p>Let\u2019s create a complex 1-dimensional tensor using the function.</p>\n<pre># Example 1<br>magnitudes = torch.tensor([1., 2])<br>angles = torch.tensor([0.25, 0.5])<br><br>complex_nums = torch.polar(magnitudes, angles)<br>complex_nums</pre>\n<pre>tensor([0.9689+0.2474j, 1.7552+0.9589j])</pre>\n<p>That worked! Let\u2019s see if we could extend this to higher dimensions as\u00a0well.</p>\n<pre># Example 2 - working<br>magnitudes = torch.tensor([[1, 2], [3, 4.]])<br>pi = np.pi<br>angles = torch.tensor([[pi/3, pi/2], [pi/4, pi/6]])<br><br>complex_nums = torch.polar(magnitudes, angles)<br>complex_nums</pre>\n<pre>tensor([[ 5.0000e-01+0.8660j, -8.7423e-08+2.0000j],<br>        [ 2.1213e+00+2.1213j,  3.4641e+00+2.0000j]])</pre>\n<p>Now, let\u2019s see what happens if the pass data of different data types to the two constituent tensors.</p>\n<pre># Example 3 - breaking<br>magnitudes = torch.tensor([[5, 12], [3, 41]], dtype = torch.float32)<br>pi = np.pi<br>angles = torch.tensor([[pi/3, pi/2], [pi/4, pi/6]], dtype = torch.float64)<br><br>complex_nums = torch.polar(magnitudes, angles)<br>complex_nums</pre>\n<pre>---------------------------------------------------------------------------<br><br>RuntimeError                              Traceback (most recent call last)<br><br>&lt;ipython-input-4-2bb46c724600&gt; in &lt;module&gt;<br>      4 angles = torch.tensor([[pi/3, pi/2], [pi/4, pi/6]], dtype = torch.float64)<br>      5 <br>----&gt; 6 complex_nums = torch.polar(magnitudes, angles)<br>      7 complex_nums<br><br><br>RuntimeError: Expected object of scalar type Float but got scalar type Double for second argument</pre>\n<p>Well, that didn\u2019t work! So, the torch.polar function expects the tensors to have the same data\u00a0types.</p>\n<p>This function is used to generate polar forms of complex numbers, which are frequently used in signal processing, which is closely related to machine learning.</p>\n<h4>Function 2\u200a\u2014\u200atorch.full_like</h4>\n<p>This function is used to generate a tensor of same dimensions as the given tensor, but filled with the value provided.</p>\n<p>In the following example, we\u2019ll create a new tensor with the same dimensions as\u00a0x.</p>\n<pre># Example 1 - working<br>x = torch.tensor([[1.,2],[3,4]])<br>y = torch.full_like(x,5)<br>y</pre>\n<pre>tensor([[5., 5.],<br>        [5., 5.]])</pre>\n<p>We see that y is of same dimensions as x, but filled with the value 5. It also inferred the same datatype from\u00a0x.</p>\n<p>Next, we\u2019ll try to explicitly mention the datatype of the new\u00a0tensor.</p>\n<pre># Example 2 - working<br>x = torch.tensor([[1.,2],[3,4]])<br>y = torch.full_like(x,5, dtype = torch.int32)<br>y</pre>\n<pre>tensor([[5, 5],<br>        [5, 5]], dtype=torch.int32)</pre>\n<p>Now, dtype is as we have instructed. Let us also see if the fill_value can be inferred by the example tensor, i.e, x. We\u2019ll not pass any fill_value.</p>\n<pre># Example 3 - breaking (to illustrate when it breaks)<br>x = torch.tensor([[1,1],[1,1]])<br>y = torch.full_like(x)<br>y</pre>\n<pre>---------------------------------------------------------------------------<br><br>TypeError                                 Traceback (most recent call last)<br><br>&lt;ipython-input-7-d44312400574&gt; in &lt;module&gt;<br>      1 # Example 3 - breaking (to illustrate when it breaks)<br>      2 x = torch.tensor([[1,1],[1,1]])<br>----&gt; 3 y = torch.full_like(x)<br>      4 y<br><br><br>TypeError: full_like() missing 1 required positional arguments: \"fill_value\"</pre>\n<p>We observe that fill value is a required argument and can\u2019t be inferred from x, despite x being filled with just 1\u00a0value.</p>\n<p>This function is extremely useful to create tensors that duplicate the dimensions of a particular tensor, but are initialized with a different value. It is especially used when the dimensions of the tensor to mimic are\u00a0unknown.</p>\n<h4>Function 3\u200a\u2014\u200atorch.bernoulli</h4>\n<p>This function is used to generate a tensor of binary numbers (i.e, 0s and 1s) using another tensor specifying the probabilities of choosing 1 in a bernoulli distribution.</p>\n<p>It takes a tensor of probabilities as input and outputs a tensor of binary numbers of the same dimensions as the\u00a0input.</p>\n<pre># Example 1 - working<br><br>a = torch.tensor([0.342, 0.765, 0.913, 0.245, 0.486])    #Let's create a tensor of probablities.<br>torch.bernoulli(a)</pre>\n<pre>tensor([1., 1., 1., 0., 1.])</pre>\n<p>We see that as expected, high probabilities correspond to 1s and low end up in 0s. The last probability being close to 0.5, could have ended up almost equally likely in either 0 or\u00a01.</p>\n<pre># Example 2 - working<br>a = torch.ones(3, 3)          # A tensor full of ones, implying that the probability of drawing \"1\" is 1<br>torch.bernoulli(a)</pre>\n<pre>tensor([[1., 1., 1.],<br>        [1., 1., 1.],<br>        [1., 1., 1.]])</pre>\n<p>In this example, the input tensor is full of ones. Therefore, the output tensor is also full of ones as the probability of choosing \u201c0\u201d is\u00a00.</p>\n<pre># Example 3 - breaking<br>a = torch.tensor([0.5, 0.2, 1.2, 0.9, 0.6])<br>torch.bernoulli(a)</pre>\n<pre>---------------------------------------------------------------------------<br><br>RuntimeError                              Traceback (most recent call last)<br><br>&lt;ipython-input-10-4476b721d566&gt; in &lt;module&gt;<br>      1 # Example 3 - breaking<br>      2 a = torch.tensor([0.5, 0.2, 1.2, 0.9, 0.6])<br>----&gt; 3 torch.bernoulli(a)<br><br><br>RuntimeError: Expected p_in &gt;= 0 &amp;&amp; p_in &lt;= 1 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)</pre>\n<p>Here, the input tensor contains the value \u201c1.2\u201d at index \u201c2\u201d, which is not a valid value for probability and therefore, the code breaks. Hence, we must ensure that all the values in the input tensor are in the range of 0 to 1 before feeding it to the torch.bernoulli function.</p>\n<p>torch.bernoulli is used to generate a bernoulli distribution of binary\u00a0numbers.</p>\n<h4>Function 4\u200a\u2014\u200atorch.topk</h4>\n<p>torch.topk function is used to find the \u2018k\u2019 largest elements in a tensor, along with their indices. It takes a tensor and a number \u2018k\u2019 as arguments.</p>\n<pre># Example 1 - working<br>a = torch.linspace(-1, 1, steps=4)    # A tensor of 4 equally spaced values in the range of [-1, 1]<br>print(a)<br>torch.topk(a, 3)</pre>\n<pre>tensor([-1.0000, -0.3333,  0.3333,  1.0000])<br><br><br><br><br><br>torch.return_types.topk(<br>values=tensor([ 1.0000,  0.3333, -0.3333]),<br>indices=tensor([3, 2, 1]))</pre>\n<p>As expected, the function returns the 3 largest numbers and their respective indices.</p>\n<pre># Example 2 - working<br>x = torch.rand(5,5)<br>print(x)<br>torch.topk(x, 3, largest = False)</pre>\n<pre>tensor([[0.9309, 0.9593, 0.3084, 0.8251, 0.5996],<br>        [0.4073, 0.1299, 0.5344, 0.2100, 0.7051],<br>        [0.5291, 0.6243, 0.2936, 0.1113, 0.4588],<br>        [0.6389, 0.6557, 0.4525, 0.3430, 0.2369],<br>        [0.6534, 0.3255, 0.9739, 0.2819, 0.3420]])<br><br><br><br><br><br>torch.return_types.topk(<br>values=tensor([[0.3084, 0.5996, 0.8251],<br>        [0.1299, 0.2100, 0.4073],<br>        [0.1113, 0.2936, 0.4588],<br>        [0.2369, 0.3430, 0.4525],<br>        [0.2819, 0.3255, 0.3420]]),<br>indices=tensor([[2, 4, 3],<br>        [1, 3, 0],<br>        [3, 2, 4],<br>        [4, 3, 2],<br>        [3, 1, 4]]))</pre>\n<p>In this example, we pass a 5x5 array to the topk function, asking for the smallest three numbers (since largest = False) along each row (since dim = 0 by default).</p>\n<p>We get the expected numbers along with their corresponding indices.</p>\n<pre># Example 3 - breaking<br>x = torch.arange(6.)     # A tensor of numbers in range [0,5]<br>print(x)<br>torch.topk(x,7)</pre>\n<pre>tensor([0., 1., 2., 3., 4., 5.])<br><br><br><br>---------------------------------------------------------------------------<br><br>RuntimeError                              Traceback (most recent call last)<br><br>&lt;ipython-input-13-200600ac317b&gt; in &lt;module&gt;<br>      2 x = torch.arange(6.)     # A tensor of numbers in range [0,5]<br>      3 print(x)<br>----&gt; 4 torch.topk(x,7)<br><br><br>RuntimeError: selected index k out of range</pre>\n<p>In this example, we ask for \u20187\u2019 largest numbers in a tensor containing only \u20186\u2019 values. Hence, the code\u00a0breaks.</p>\n<p>This function is pretty useful when we are only interested in either the few largest or smallest numbers in a tensor, irrespective of its\u00a0size.</p>\n<h4>Function 5\u200a\u2014\u200atorch.combinations</h4>\n<p>This function is used to generate a tensor of all combinations of size \u2018r\u2019 from a tensor. By default, it does it without replacements.</p>\n<pre># Example 1 - working<br>a = torch.tensor([1,2,3,4])<br>torch.combinations(a,3)</pre>\n<pre>tensor([[1, 2, 3],<br>        [1, 2, 4],<br>        [1, 3, 4],<br>        [2, 3, 4]])</pre>\n<p>It generated the 4 possible combinations of 3 elements from the given tensor as expected.</p>\n<pre># Example 2 - working<br>a = torch.tensor([1,2,3])<br>torch.combinations(a,2, with_replacement = True)</pre>\n<pre>tensor([[1, 1],<br>        [1, 2],<br>        [1, 3],<br>        [2, 2],<br>        [2, 3],<br>        [3, 3]])</pre>\n<p>Now, with \u2018with_replacement\u2019 parameter set to True, we see that replacements are allowed, giving many more combinations.</p>\n<pre># Example 3 - breaking<br>a = torch.tensor([[1,2,3], [4,5,6]])<br>torch.combinations(a,2)</pre>\n<pre>---------------------------------------------------------------------------<br><br>RuntimeError                              Traceback (most recent call last)<br><br>&lt;ipython-input-16-a9568cd81328&gt; in &lt;module&gt;<br>      1 # Example 3 - breaking<br>      2 a = torch.tensor([[1,2,3], [4,5,6]])<br>----&gt; 3 torch.combinations(a,2)<br><br><br>RuntimeError: Expect a 1D vector, but got shape [2, 3]</pre>\n<p>Here, when we provided a multi-dimensional vector as input, it is ambiguous as to how to choose elements for combinations, and hence, the code\u00a0breaks.</p>\n<p>This function is very useful to create all possible combinations of elements present in the input tensor, which is a common use case in discrete\u00a0math.</p>\n<h4>Conclusion</h4>\n<p>In this notebook, we had a sneak peek at 5 cool and useful functions in PyTorch. These functions are pretty useful for programmers working in Machine Learning to quickly generate the data in the way they want without worrying about the underlying logic involved.</p>\n<p>There are many more such functions in PyTorch, which can be explored in the PyTorch documentation, the link to which is provided below. Keep exploring!</p>\n<h4>Reference Link</h4>\n<ul><li>Official documentation for tensor operations: <a href=\"https://pytorch.org/docs/stable/torch.html\">https://pytorch.org/docs/stable/torch.html</a>\n</li></ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f8ad67c2779b\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["python","data-science","machine-learning","deep-learning","pytorch"]}]}